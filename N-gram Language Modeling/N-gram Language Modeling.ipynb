{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS EXAMPLE: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', 'the', 'jury', 'further', 'said', 'in']\n",
      "\n",
      "\n",
      "VOCAB EXAMPLE: ['drudgery', 'one-arm', 'growling', 'cutest', 'rain', 'hops', \"network's\", 'expressionists', 'polarization', 'gaussian']\n"
     ]
    }
   ],
   "source": [
    "# Loading the corpus\n",
    "corpus = brown.words()\n",
    "\n",
    "# Case folding and getting vocab\n",
    "lower_case_corpus = [w.lower() for w in corpus]\n",
    "vocab = set(lower_case_corpus)\n",
    "\n",
    "print('CORPUS EXAMPLE: ' + str(lower_case_corpus[:30]) + '\\n\\n')\n",
    "print('VOCAB EXAMPLE: ' + str(list(vocab)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Corpus: 1161192\n",
      "Vocab of the Corpus: 49815\n"
     ]
    }
   ],
   "source": [
    "print('Total words in Corpus: ' + str(len(lower_case_corpus)))\n",
    "print('Vocab of the Corpus: ' + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example, count for bigram ('the', 'king') is: 51\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = {}\n",
    "trigram_counts = {}\n",
    "\n",
    "# Sliding through corpus to get bigram and trigram counts\n",
    "for i in range(len(lower_case_corpus) - 2):\n",
    "    # Getting bigram and trigram at each slide\n",
    "    bigram = (lower_case_corpus[i], lower_case_corpus[i+1])\n",
    "    trigram = (lower_case_corpus[i], lower_case_corpus[i+1], lower_case_corpus[i+2])\n",
    "    \n",
    "    # Keeping track of the bigram counts\n",
    "    if bigram in bigram_counts.keys():\n",
    "        bigram_counts[bigram] += 1\n",
    "    else:\n",
    "        bigram_counts[bigram] = 1\n",
    "    \n",
    "    # Keeping track of trigram counts\n",
    "    if trigram in trigram_counts.keys():\n",
    "        trigram_counts[trigram] += 1\n",
    "    else:\n",
    "        trigram_counts[trigram] = 1\n",
    "\n",
    "print(\"Example, count for bigram ('the', 'king') is: \" + str(bigram_counts[('the', 'king')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function takes sentence as input and suggests possible words that comes after the sentence  \n",
    "def suggest_next_word(input_, bigram_counts, trigram_counts, vocab):\n",
    "    # Consider the last bigram of sentence\n",
    "    tokenized_input = word_tokenize(input_.lower())\n",
    "    last_bigram = tokenized_input[-2:]\n",
    "    \n",
    "    # Calculating probability for each word in vocab\n",
    "    vocab_probabilities = {}\n",
    "    for vocab_word in vocab:\n",
    "        test_trigram = (last_bigram[0], last_bigram[1], vocab_word)\n",
    "        test_bigram = (last_bigram[0], last_bigram[1])\n",
    "\n",
    "        test_trigram_count = trigram_counts.get(test_trigram, 0)\n",
    "        test_bigram_count = bigram_counts.get(test_bigram, 0)\n",
    "        \n",
    "        probability = test_trigram_count / test_bigram_count\n",
    "        vocab_probabilities[vocab_word] = probability\n",
    "    \n",
    "    # Sorting the vocab probability in descending order to get top probable words\n",
    "    top_suggestions = sorted(vocab_probabilities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    return top_suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('james', 0.17647058823529413),\n",
       " ('of', 0.1568627450980392),\n",
       " ('arthur', 0.11764705882352941)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word('I am the king', bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 0.3333333333333333),\n",
       " ('hearts', 0.16666666666666666),\n",
       " ('morocco', 0.08333333333333333)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word('I am the king of', bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0.26666666666666666), ('.', 0.26666666666666666), (',', 0.2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word('I am the king of france', bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0.2), ('germany', 0.13333333333333333), ('some', 0.06666666666666667)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_next_word('I am the king of france and', bigram_counts, trigram_counts, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Tweaks\n",
    "\n",
    "* Adding corpus (nltk, scrapping, etc)\n",
    "* DIfferent model better than trigram model\n",
    "* Handling 0 counts in model (Smoothing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
